import os
import shutil

from datetime import datetime
from pathlib import Path
from airflow import DAG
from airflow.operators import CoutureSparkOperator
from airflow.operators.bash_operator import BashOperator
from airflow.operators.python_operator import BranchPythonOperator, PythonOperator
from airflow.settings import EDA_HOME


classPath = 'ai.couture.obelisk.eda.MainClass'
appName = 'EDAUniVariateBriefDataSummary'
code_artifact = 'obelisk-eda.jar'

# Database testing URI
# uripath = "mysql://couture:couture@mysql/couture"
# file_name = "dag_run"

# HDFS testing URI
# uri = 'hdfs://data/ecomm/ajio/processed/productAttributes'
uri = '{{source.connection_uri}}'
file_name = uri.split("/")[-1]
file_folder = file_name.split(".")[0]
uripath = uri.split(":/")[1].replace(file_name, "")

folder_to_copy_summary = '{{folder_to_copy_sum}}'
rawDirPath = uripath


def get_source():
    if "hdfs" in uri:
        source = "CreateOutputHDFSPathIfNotExists"
    else:
        source = "GenerateSummaryDB"
    return source


source = get_source()


def get_hdfs_processed_dir_path():
    pth = str(Path('/data/eda/processed/outputfiles/').joinpath(*[folder_to_copy_summary, "preliminary"]))

    if not pth.endswith('/'):
        pth += '/'
    return pth


HdfsProcessedDirPath = get_hdfs_processed_dir_path()

# local file path is the file path on the webserver, where we want to output
# keep preliminary data

# This path is the same as data_dump_dir in EDA_preliminary_visualisations
data_dump_dir = Path(EDA_HOME).joinpath(*['inputs', folder_to_copy_summary, 'preliminary'])

data_dump_dir_hdfs = Path(EDA_HOME).joinpath(*["inputs", folder_to_copy_summary])


# def mkdir_dirs(ds, **kwargs):
#     if source == "GenerateSummaryDB":
#         data_dump_dir.mkdir(parents=True, exist_ok=True)
#     else:
#         data_dump_dir_hdfs.mkdir(parents=True, exist_ok=True)


default_args = {
    'owner': '{{username}}',
    'depends_on_past': False,
    'start_date': datetime({{now.year}}, {{now.month}}, {{now.day}}),
    'retries': 0,
}

EDADag = DAG(
    '{{dag_id}}',
    default_args=default_args,
    schedule_interval=None)

# EDAPreliminaryCreateDirs = PythonOperator(
#     task_id='CreateDirs',
#     provide_context=True,
#     python_callable=mkdir_dirs,
#     dag=EDADag
# )

branch_task = BranchPythonOperator(
    task_id='branching',
    python_callable=get_source,
    dag=EDADag,
)

CreateOutputHDFSPathIfNotExists = BashOperator(
    task_id='CreateOutputHDFSPathIfNotExists',
    dag=EDADag,
    description='',
    bash_command='hdfs dfs -mkdir -p {}'.format(Path(HdfsProcessedDirPath).parent)
)

GeneratePreliminarySummaryHDFS = CoutureSparkOperator(
    task_id='GeneratePreliminarySummaryHDFS',
    app_name=appName,
    class_path=classPath,
    code_artifact=code_artifact,
    input_base_dir_path=str(rawDirPath),
    output_base_dir_path=str(HdfsProcessedDirPath),
    input_filenames_dict={"data": str(file_name)},
    output_filenames_dict={"preliminary_data_summary": "preliminary_summary"},
    dag=EDADag,
    description='')


# only used with "GeneratePreliminarySummaryDB"
op_kwargs = {
    'uri': '{{source.connection_uri}}',
    'local_output_path': data_dump_dir,
    'table_name': '{{source.tablename}}'
}

GeneratePreliminarySummaryDB = BashOperator(
    task_id='GenerateSummaryDB',
    depends_on_past=False,
    bash_command='python3 "{}" "{}" "{}" "{}"'.format(
        os.path.join(EDA_HOME, *["codes", "preliminary_summary_db.py"]),
        op_kwargs['uri'], op_kwargs['table_name'],
        op_kwargs['local_output_path']),
    dag=EDADag
)

GetTableInfo = CoutureSparkOperator(
    task_id='GetTableInformation',
    app_name=appName,
    class_path=classPath,
    code_artifact=code_artifact,
    input_base_dir_path=str(rawDirPath),
    output_base_dir_path=str(HdfsProcessedDirPath),
    input_filenames_dict={"data": str(file_name)},
    output_filenames_dict={"data_info": "table_info"},
    dag=EDADag,
    description='')


del_directory_cmd = 'rm -r {}'.format(Path(data_dump_dir_hdfs))
copy_directory_cmd = 'hdfs dfs -get {} {}'.format(Path(HdfsProcessedDirPath).parent, data_dump_dir_hdfs)
cmd = f'{del_directory_cmd} && {copy_directory_cmd}'

CopyFilesToLocal = BashOperator(
    task_id='CopySummaryData',
    dag=EDADag,
    description='',
    bash_command=cmd
)


def conditionally_trigger(context, dag_run_obj):
    c_p = context['params']['condition_param']
    print("Controller DAG : conditionally_trigger = {}".format(c_p))
    if context['params']['condition_param']:
        dag_run_obj.payload = {'message': context['params']['message']}
        # pp.pprint(dag_run_obj.payload)
        return dag_run_obj


# branch_task.set_upstream(EDAPreliminaryCreateDirs)
CreateOutputHDFSPathIfNotExists.set_upstream(branch_task)
GeneratePreliminarySummaryHDFS.set_upstream(CreateOutputHDFSPathIfNotExists)
GetTableInfo.set_upstream(GeneratePreliminarySummaryHDFS)
CopyFilesToLocal.set_upstream(GetTableInfo)

GeneratePreliminarySummaryDB.set_upstream(branch_task)
